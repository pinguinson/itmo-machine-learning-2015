{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(15000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 15 seconds\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 5\n",
    "%autosave 15\n",
    "\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>feat_1</th>\n",
       "      <th>feat_2</th>\n",
       "      <th>feat_3</th>\n",
       "      <th>feat_4</th>\n",
       "      <th>feat_5</th>\n",
       "      <th>feat_6</th>\n",
       "      <th>feat_7</th>\n",
       "      <th>feat_8</th>\n",
       "      <th>feat_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_84</th>\n",
       "      <th>feat_85</th>\n",
       "      <th>feat_86</th>\n",
       "      <th>feat_87</th>\n",
       "      <th>feat_88</th>\n",
       "      <th>feat_89</th>\n",
       "      <th>feat_90</th>\n",
       "      <th>feat_91</th>\n",
       "      <th>feat_92</th>\n",
       "      <th>feat_93</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td> 61878.000000</td>\n",
       "      <td> 61878.00000</td>\n",
       "      <td> 61878.000000</td>\n",
       "      <td> 61878.000000</td>\n",
       "      <td> 61878.000000</td>\n",
       "      <td> 61878.000000</td>\n",
       "      <td> 61878.000000</td>\n",
       "      <td> 61878.000000</td>\n",
       "      <td> 61878.000000</td>\n",
       "      <td> 61878.000000</td>\n",
       "      <td>...</td>\n",
       "      <td> 61878.000000</td>\n",
       "      <td> 61878.000000</td>\n",
       "      <td> 61878.000000</td>\n",
       "      <td> 61878.000000</td>\n",
       "      <td> 61878.000000</td>\n",
       "      <td> 61878.000000</td>\n",
       "      <td> 61878.000000</td>\n",
       "      <td> 61878.000000</td>\n",
       "      <td> 61878.000000</td>\n",
       "      <td> 61878.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td> 30939.500000</td>\n",
       "      <td>     0.38668</td>\n",
       "      <td>     0.263066</td>\n",
       "      <td>     0.901467</td>\n",
       "      <td>     0.779081</td>\n",
       "      <td>     0.071043</td>\n",
       "      <td>     0.025696</td>\n",
       "      <td>     0.193704</td>\n",
       "      <td>     0.662433</td>\n",
       "      <td>     1.011296</td>\n",
       "      <td>...</td>\n",
       "      <td>     0.070752</td>\n",
       "      <td>     0.532306</td>\n",
       "      <td>     1.128576</td>\n",
       "      <td>     0.393549</td>\n",
       "      <td>     0.874915</td>\n",
       "      <td>     0.457772</td>\n",
       "      <td>     0.812421</td>\n",
       "      <td>     0.264941</td>\n",
       "      <td>     0.380119</td>\n",
       "      <td>     0.126135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td> 17862.784315</td>\n",
       "      <td>     1.52533</td>\n",
       "      <td>     1.252073</td>\n",
       "      <td>     2.934818</td>\n",
       "      <td>     2.788005</td>\n",
       "      <td>     0.438902</td>\n",
       "      <td>     0.215333</td>\n",
       "      <td>     1.030102</td>\n",
       "      <td>     2.255770</td>\n",
       "      <td>     3.474822</td>\n",
       "      <td>...</td>\n",
       "      <td>     1.151460</td>\n",
       "      <td>     1.900438</td>\n",
       "      <td>     2.681554</td>\n",
       "      <td>     1.575455</td>\n",
       "      <td>     2.115466</td>\n",
       "      <td>     1.527385</td>\n",
       "      <td>     4.597804</td>\n",
       "      <td>     2.045646</td>\n",
       "      <td>     0.982385</td>\n",
       "      <td>     1.201720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>     1.000000</td>\n",
       "      <td>     0.00000</td>\n",
       "      <td>     0.000000</td>\n",
       "      <td>     0.000000</td>\n",
       "      <td>     0.000000</td>\n",
       "      <td>     0.000000</td>\n",
       "      <td>     0.000000</td>\n",
       "      <td>     0.000000</td>\n",
       "      <td>     0.000000</td>\n",
       "      <td>     0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>     0.000000</td>\n",
       "      <td>     0.000000</td>\n",
       "      <td>     0.000000</td>\n",
       "      <td>     0.000000</td>\n",
       "      <td>     0.000000</td>\n",
       "      <td>     0.000000</td>\n",
       "      <td>     0.000000</td>\n",
       "      <td>     0.000000</td>\n",
       "      <td>     0.000000</td>\n",
       "      <td>     0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td> 15470.250000</td>\n",
       "      <td>     0.00000</td>\n",
       "      <td>     0.000000</td>\n",
       "      <td>     0.000000</td>\n",
       "      <td>     0.000000</td>\n",
       "      <td>     0.000000</td>\n",
       "      <td>     0.000000</td>\n",
       "      <td>     0.000000</td>\n",
       "      <td>     0.000000</td>\n",
       "      <td>     0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>     0.000000</td>\n",
       "      <td>     0.000000</td>\n",
       "      <td>     0.000000</td>\n",
       "      <td>     0.000000</td>\n",
       "      <td>     0.000000</td>\n",
       "      <td>     0.000000</td>\n",
       "      <td>     0.000000</td>\n",
       "      <td>     0.000000</td>\n",
       "      <td>     0.000000</td>\n",
       "      <td>     0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td> 30939.500000</td>\n",
       "      <td>     0.00000</td>\n",
       "      <td>     0.000000</td>\n",
       "      <td>     0.000000</td>\n",
       "      <td>     0.000000</td>\n",
       "      <td>     0.000000</td>\n",
       "      <td>     0.000000</td>\n",
       "      <td>     0.000000</td>\n",
       "      <td>     0.000000</td>\n",
       "      <td>     0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>     0.000000</td>\n",
       "      <td>     0.000000</td>\n",
       "      <td>     0.000000</td>\n",
       "      <td>     0.000000</td>\n",
       "      <td>     0.000000</td>\n",
       "      <td>     0.000000</td>\n",
       "      <td>     0.000000</td>\n",
       "      <td>     0.000000</td>\n",
       "      <td>     0.000000</td>\n",
       "      <td>     0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td> 46408.750000</td>\n",
       "      <td>     0.00000</td>\n",
       "      <td>     0.000000</td>\n",
       "      <td>     0.000000</td>\n",
       "      <td>     0.000000</td>\n",
       "      <td>     0.000000</td>\n",
       "      <td>     0.000000</td>\n",
       "      <td>     0.000000</td>\n",
       "      <td>     1.000000</td>\n",
       "      <td>     0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>     0.000000</td>\n",
       "      <td>     0.000000</td>\n",
       "      <td>     1.000000</td>\n",
       "      <td>     0.000000</td>\n",
       "      <td>     1.000000</td>\n",
       "      <td>     0.000000</td>\n",
       "      <td>     0.000000</td>\n",
       "      <td>     0.000000</td>\n",
       "      <td>     0.000000</td>\n",
       "      <td>     0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td> 61878.000000</td>\n",
       "      <td>    61.00000</td>\n",
       "      <td>    51.000000</td>\n",
       "      <td>    64.000000</td>\n",
       "      <td>    70.000000</td>\n",
       "      <td>    19.000000</td>\n",
       "      <td>    10.000000</td>\n",
       "      <td>    38.000000</td>\n",
       "      <td>    76.000000</td>\n",
       "      <td>    43.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>    76.000000</td>\n",
       "      <td>    55.000000</td>\n",
       "      <td>    65.000000</td>\n",
       "      <td>    67.000000</td>\n",
       "      <td>    30.000000</td>\n",
       "      <td>    61.000000</td>\n",
       "      <td>   130.000000</td>\n",
       "      <td>    52.000000</td>\n",
       "      <td>    19.000000</td>\n",
       "      <td>    87.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 94 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id       feat_1        feat_2        feat_3        feat_4  \\\n",
       "count  61878.000000  61878.00000  61878.000000  61878.000000  61878.000000   \n",
       "mean   30939.500000      0.38668      0.263066      0.901467      0.779081   \n",
       "std    17862.784315      1.52533      1.252073      2.934818      2.788005   \n",
       "min        1.000000      0.00000      0.000000      0.000000      0.000000   \n",
       "25%    15470.250000      0.00000      0.000000      0.000000      0.000000   \n",
       "50%    30939.500000      0.00000      0.000000      0.000000      0.000000   \n",
       "75%    46408.750000      0.00000      0.000000      0.000000      0.000000   \n",
       "max    61878.000000     61.00000     51.000000     64.000000     70.000000   \n",
       "\n",
       "             feat_5        feat_6        feat_7        feat_8        feat_9  \\\n",
       "count  61878.000000  61878.000000  61878.000000  61878.000000  61878.000000   \n",
       "mean       0.071043      0.025696      0.193704      0.662433      1.011296   \n",
       "std        0.438902      0.215333      1.030102      2.255770      3.474822   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      1.000000      0.000000   \n",
       "max       19.000000     10.000000     38.000000     76.000000     43.000000   \n",
       "\n",
       "           ...            feat_84       feat_85       feat_86       feat_87  \\\n",
       "count      ...       61878.000000  61878.000000  61878.000000  61878.000000   \n",
       "mean       ...           0.070752      0.532306      1.128576      0.393549   \n",
       "std        ...           1.151460      1.900438      2.681554      1.575455   \n",
       "min        ...           0.000000      0.000000      0.000000      0.000000   \n",
       "25%        ...           0.000000      0.000000      0.000000      0.000000   \n",
       "50%        ...           0.000000      0.000000      0.000000      0.000000   \n",
       "75%        ...           0.000000      0.000000      1.000000      0.000000   \n",
       "max        ...          76.000000     55.000000     65.000000     67.000000   \n",
       "\n",
       "            feat_88       feat_89       feat_90       feat_91       feat_92  \\\n",
       "count  61878.000000  61878.000000  61878.000000  61878.000000  61878.000000   \n",
       "mean       0.874915      0.457772      0.812421      0.264941      0.380119   \n",
       "std        2.115466      1.527385      4.597804      2.045646      0.982385   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        1.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max       30.000000     61.000000    130.000000     52.000000     19.000000   \n",
       "\n",
       "            feat_93  \n",
       "count  61878.000000  \n",
       "mean       0.126135  \n",
       "std        1.201720  \n",
       "min        0.000000  \n",
       "25%        0.000000  \n",
       "50%        0.000000  \n",
       "75%        0.000000  \n",
       "max       87.000000  \n",
       "\n",
       "[8 rows x 94 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('Data/train.csv')\n",
    "train_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=400, max_depth=15, min_samples_split=12, min_samples_leaf=3, max_features=20)\n",
    "rf_best_params = {\"n_estimators\": [450,500], \n",
    "                  \"max_depth\": [10,15,20], \n",
    "                  \"min_samples_split\": [6,13], \n",
    "                  \"min_samples_leaf\": [1,3],\n",
    "                  \"max_features\": [20,30,40]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data = train_data.drop(\"id\", axis = 1)\n",
    "def convert_target(x):\n",
    "    return int(x[\"target\"].split('_')[1])\n",
    "train_data[\"target\"] = train_data.apply(convert_target, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "import datetime, time\n",
    "def test_method(method, a_train, a_test, b_train, b_test):\n",
    "    print method.__class__\n",
    "    method.fit(a_train, b_train)\n",
    "    print log_loss(b_train, method.predict_proba(a_train))\n",
    "\n",
    "def test_methods(methods, a_train, a_test, b_train, b_test):\n",
    "    for method in methods:\n",
    "        test_method(method, a_train, a_test, b_train, b_test)\n",
    "\n",
    "def test_models(data):\n",
    "    target = data.target\n",
    "    train = data.drop('target', axis = 1)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(train, target, test_size = 0.2, random_state = 42)\n",
    "    methods = [RandomForestClassifier(n_estimators = 100)]\n",
    "    test_methods(methods, X_train, X_test, y_train, y_test)\n",
    "\n",
    "def get_best_params(method, params, data):\n",
    "    target = data.target\n",
    "    train = data.drop('target', axis = 1)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(train, target, test_size = 0.2, random_state = 42)\n",
    "    errors_train, errors_test = aggregate_train_test_errors(method, params, X_train, y_train, X_test, y_test, [log_loss], print_flag=True)\n",
    "    plot_train_test_errors(method, params, errors_train, errors_test)\n",
    "    \n",
    "def print_score(model, metric, X_train, y_train, X_test, y_test):\n",
    "        print(\"Model: %s Metric: %s On test: %f\" % (model.__class__.__name__, metric.__name__, metric(y_test, model.predict_proba(X_test))))\n",
    "        \n",
    "def print_scores(model, metrics, X_train, y_train, X_test, y_test, parameter = None):\n",
    "    for metric in metrics:\n",
    "        if parameter != None:\n",
    "            print(\"%s %s\" % (parameter[0], parameter[1]))\n",
    "            print_score(model, metric, X_train, y_train, X_test, y_test)\n",
    "        else:\n",
    "            print_score(model, metric, X_train, y_train, X_test, y_test)\n",
    "    \n",
    "def aggregate_train_test_errors(model, params, X_train, y_train, X_test, y_test, metrics = [], print_flag = False):\n",
    "    errors_train = {key: {'params': params[key], 'values': []} for key in params.keys()}\n",
    "    errors_test = {key: {'params': params[key], 'values': []}  for key in params.keys()}\n",
    "    base_parameters = model.get_params()\n",
    "    start_time = time.time()\n",
    "    prev_time = start_time\n",
    "    for key, values in params.iteritems():\n",
    "        if isinstance(values, list):\n",
    "            for value in values:\n",
    "                model.set_params(**{key: value})\n",
    "                model.fit(X_train, y_train)\n",
    "                cur_time = time.time()\n",
    "                from_start = datetime.datetime.fromtimestamp(cur_time - start_time + time.timezone).strftime('%H:%M:%S')\n",
    "                from_last = datetime.datetime.fromtimestamp(cur_time - prev_time + time.timezone).strftime('%H:%M:%S')\n",
    "                print from_start + \" (\" + from_last + \")\"\n",
    "                prev_time = cur_time\n",
    "                errors_train[key]['values'].append(model.score(X_train, y_train))\n",
    "                errors_test[key]['values'].append(model.score(X_test, y_test))\n",
    "                if print_flag == True and metrics:\n",
    "                    print_scores(model, metrics, X_train, y_train, X_test, y_test, (key, value))\n",
    "            model.set_params(**base_parameters)\n",
    "        else:\n",
    "            raise Exception(\"Take only list of parameters!\")   \n",
    "    return errors_train, errors_test\n",
    "\n",
    "def plot_train_test_errors(model, model_params, errors_train, errors_test, save = None):\n",
    "    fig, axes = plt.subplots(ncols=len(model_params))\n",
    "    fig.set_size_inches((15,9))\n",
    "    for ind, param in enumerate(model_params.keys()):\n",
    "        if isinstance(errors_train[param]['params'][0], str):\n",
    "            axes[ind].plot(range(len(errors_train[param]['params'])), errors_train[param]['values'], label = 'on train')\n",
    "            axes[ind].plot(range(len(errors_train[param]['params'])), errors_test[param]['values'], label = 'on test')\n",
    "            axes[ind].xaxis.set_ticks(range(len(errors_train[param]['params'])), errors_train[param]['params'])\n",
    "        else:            \n",
    "            axes[ind].plot(errors_train[param]['params'], errors_train[param]['values'], label = 'on train')\n",
    "            axes[ind].plot(errors_train[param]['params'], errors_test[param]['values'], label = 'on test')\n",
    "            axes[ind].xaxis.set_ticks(range(len(errors_train[param]['params'])), errors_train[param]['params'])\n",
    "        axes[ind].set_title(str(param))\n",
    "    plt.legend()\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:00:43\n",
      "n_estimators 10\n",
      "Model: GradientBoostingClassifier Metric: log_loss On test: 1.067229\n",
      "00:04:04\n",
      "n_estimators 50\n",
      "Model: GradientBoostingClassifier Metric: log_loss On test: 0.665445\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-5f85ff2ec73a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m              \"max_features\": range(10, 100, 10)}\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mget_best_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGradientBoostingClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrf_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-21-b766305e4a90>\u001b[0m in \u001b[0;36mget_best_params\u001b[1;34m(method, params, data)\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'target'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0merrors_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maggregate_train_test_errors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlog_loss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_flag\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[0mplot_train_test_errors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-b766305e4a90>\u001b[0m in \u001b[0;36maggregate_train_test_errors\u001b[1;34m(model, params, X_train, y_train, X_test, y_test, metrics, print_flag)\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m                 \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m                 \u001b[1;32mprint\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfromtimestamp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimezone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%H:%M:%S'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m                 \u001b[0merrors_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'values'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/pinguinson/anaconda/lib/python2.7/site-packages/sklearn/ensemble/gradient_boosting.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, monitor)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1129\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_classes_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1130\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGradientBoostingClassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_score_to_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/pinguinson/anaconda/lib/python2.7/site-packages/sklearn/ensemble/gradient_boosting.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, monitor)\u001b[0m\n\u001b[0;32m    781\u001b[0m         \u001b[1;31m# fit the boosting stages\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m         n_stages = self._fit_stages(X, y, y_pred, random_state,\n\u001b[1;32m--> 783\u001b[1;33m                                     begin_at_stage, monitor)\n\u001b[0m\u001b[0;32m    784\u001b[0m         \u001b[1;31m# change shape of arrays after fit (early-stopping or additional ests)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn_stages\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/pinguinson/anaconda/lib/python2.7/site-packages/sklearn/ensemble/gradient_boosting.pyc\u001b[0m in \u001b[0;36m_fit_stages\u001b[1;34m(self, X, y, y_pred, random_state, begin_at_stage, monitor)\u001b[0m\n\u001b[0;32m    833\u001b[0m             \u001b[1;31m# fit next stage of trees\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    834\u001b[0m             y_pred = self._fit_stage(i, X, y, y_pred, sample_mask,\n\u001b[1;32m--> 835\u001b[1;33m                                      criterion, splitter, random_state)\n\u001b[0m\u001b[0;32m    836\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    837\u001b[0m             \u001b[1;31m# track deviance (= loss)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/pinguinson/anaconda/lib/python2.7/site-packages/sklearn/ensemble/gradient_boosting.pyc\u001b[0m in \u001b[0;36m_fit_stage\u001b[1;34m(self, i, X, y, y_pred, sample_mask, criterion, splitter, random_state)\u001b[0m\n\u001b[0;32m    570\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m             tree.fit(X, residual,\n\u001b[1;32m--> 572\u001b[1;33m                      sample_weight=sample_weight, check_input=False)\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m             \u001b[1;31m# update tree leaves\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/pinguinson/anaconda/lib/python2.7/site-packages/sklearn/tree/tree.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_mask, X_argsorted, check_input, sample_weight)\u001b[0m\n\u001b[0;32m    265\u001b[0m                                            max_leaf_nodes)\n\u001b[0;32m    266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 267\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    268\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rf_params = {\"n_estimators\": [10,50,150,300,450,600], \n",
    "             \"max_depth\": range(1,26, 2), \n",
    "             \"min_samples_split\": range(1,26, 2), \n",
    "             \"min_samples_leaf\": range(1,26, 2), \n",
    "             \"max_features\": range(10, 100, 10)}\n",
    "\n",
    "get_best_params(GradientBoostingClassifier(), rf_params, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target = train_data['target'].values\n",
    "train = train_data.drop(['target'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('Data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = test.drop(\"id\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "clf = ExtraTreesClassifier(n_estimators=200, min_samples_leaf=10, max_features=30, criterion=\"gini\", min_samples_split=3, n_jobs=4)\n",
    "clf.fit(train, target)\n",
    "predicted = clf.predict_proba(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sub = pd.DataFrame(predicted, columns = ['Class_1','Class_2','Class_3','Class_4','Class_5','Class_6','Class_7','Class_8','Class_9'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class_1</th>\n",
       "      <th>Class_2</th>\n",
       "      <th>Class_3</th>\n",
       "      <th>Class_4</th>\n",
       "      <th>Class_5</th>\n",
       "      <th>Class_6</th>\n",
       "      <th>Class_7</th>\n",
       "      <th>Class_8</th>\n",
       "      <th>Class_9</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td> 0.016149</td>\n",
       "      <td> 0.298917</td>\n",
       "      <td> 0.237240</td>\n",
       "      <td> 0.348643</td>\n",
       "      <td> 0.003502</td>\n",
       "      <td> 0.023315</td>\n",
       "      <td> 0.044615</td>\n",
       "      <td> 0.015468</td>\n",
       "      <td> 0.012150</td>\n",
       "      <td> 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td> 0.020050</td>\n",
       "      <td> 0.087136</td>\n",
       "      <td> 0.032367</td>\n",
       "      <td> 0.015229</td>\n",
       "      <td> 0.005956</td>\n",
       "      <td> 0.533094</td>\n",
       "      <td> 0.021089</td>\n",
       "      <td> 0.251825</td>\n",
       "      <td> 0.033253</td>\n",
       "      <td> 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td> 0.002127</td>\n",
       "      <td> 0.001288</td>\n",
       "      <td> 0.001146</td>\n",
       "      <td> 0.000858</td>\n",
       "      <td> 0.001970</td>\n",
       "      <td> 0.948176</td>\n",
       "      <td> 0.008132</td>\n",
       "      <td> 0.034155</td>\n",
       "      <td> 0.002148</td>\n",
       "      <td> 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td> 0.005149</td>\n",
       "      <td> 0.555534</td>\n",
       "      <td> 0.275743</td>\n",
       "      <td> 0.067138</td>\n",
       "      <td> 0.000278</td>\n",
       "      <td> 0.005573</td>\n",
       "      <td> 0.008840</td>\n",
       "      <td> 0.007268</td>\n",
       "      <td> 0.074479</td>\n",
       "      <td> 4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td> 0.103240</td>\n",
       "      <td> 0.007701</td>\n",
       "      <td> 0.004552</td>\n",
       "      <td> 0.000625</td>\n",
       "      <td> 0.000417</td>\n",
       "      <td> 0.043724</td>\n",
       "      <td> 0.036427</td>\n",
       "      <td> 0.397412</td>\n",
       "      <td> 0.405901</td>\n",
       "      <td> 5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Class_1   Class_2   Class_3   Class_4   Class_5   Class_6   Class_7  \\\n",
       "0  0.016149  0.298917  0.237240  0.348643  0.003502  0.023315  0.044615   \n",
       "1  0.020050  0.087136  0.032367  0.015229  0.005956  0.533094  0.021089   \n",
       "2  0.002127  0.001288  0.001146  0.000858  0.001970  0.948176  0.008132   \n",
       "3  0.005149  0.555534  0.275743  0.067138  0.000278  0.005573  0.008840   \n",
       "4  0.103240  0.007701  0.004552  0.000625  0.000417  0.043724  0.036427   \n",
       "\n",
       "    Class_8   Class_9  id  \n",
       "0  0.015468  0.012150   1  \n",
       "1  0.251825  0.033253   2  \n",
       "2  0.034155  0.002148   3  \n",
       "3  0.007268  0.074479   4  \n",
       "4  0.397412  0.405901   5  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub[\"id\"] = list(range(1,len(sub.index) + 1))\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cols = sub.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "sub = sub[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Class_1</th>\n",
       "      <th>Class_2</th>\n",
       "      <th>Class_3</th>\n",
       "      <th>Class_4</th>\n",
       "      <th>Class_5</th>\n",
       "      <th>Class_6</th>\n",
       "      <th>Class_7</th>\n",
       "      <th>Class_8</th>\n",
       "      <th>Class_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td> 1</td>\n",
       "      <td> 0.016149</td>\n",
       "      <td> 0.298917</td>\n",
       "      <td> 0.237240</td>\n",
       "      <td> 0.348643</td>\n",
       "      <td> 0.003502</td>\n",
       "      <td> 0.023315</td>\n",
       "      <td> 0.044615</td>\n",
       "      <td> 0.015468</td>\n",
       "      <td> 0.012150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td> 2</td>\n",
       "      <td> 0.020050</td>\n",
       "      <td> 0.087136</td>\n",
       "      <td> 0.032367</td>\n",
       "      <td> 0.015229</td>\n",
       "      <td> 0.005956</td>\n",
       "      <td> 0.533094</td>\n",
       "      <td> 0.021089</td>\n",
       "      <td> 0.251825</td>\n",
       "      <td> 0.033253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td> 3</td>\n",
       "      <td> 0.002127</td>\n",
       "      <td> 0.001288</td>\n",
       "      <td> 0.001146</td>\n",
       "      <td> 0.000858</td>\n",
       "      <td> 0.001970</td>\n",
       "      <td> 0.948176</td>\n",
       "      <td> 0.008132</td>\n",
       "      <td> 0.034155</td>\n",
       "      <td> 0.002148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td> 4</td>\n",
       "      <td> 0.005149</td>\n",
       "      <td> 0.555534</td>\n",
       "      <td> 0.275743</td>\n",
       "      <td> 0.067138</td>\n",
       "      <td> 0.000278</td>\n",
       "      <td> 0.005573</td>\n",
       "      <td> 0.008840</td>\n",
       "      <td> 0.007268</td>\n",
       "      <td> 0.074479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td> 5</td>\n",
       "      <td> 0.103240</td>\n",
       "      <td> 0.007701</td>\n",
       "      <td> 0.004552</td>\n",
       "      <td> 0.000625</td>\n",
       "      <td> 0.000417</td>\n",
       "      <td> 0.043724</td>\n",
       "      <td> 0.036427</td>\n",
       "      <td> 0.397412</td>\n",
       "      <td> 0.405901</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id   Class_1   Class_2   Class_3   Class_4   Class_5   Class_6   Class_7  \\\n",
       "0   1  0.016149  0.298917  0.237240  0.348643  0.003502  0.023315  0.044615   \n",
       "1   2  0.020050  0.087136  0.032367  0.015229  0.005956  0.533094  0.021089   \n",
       "2   3  0.002127  0.001288  0.001146  0.000858  0.001970  0.948176  0.008132   \n",
       "3   4  0.005149  0.555534  0.275743  0.067138  0.000278  0.005573  0.008840   \n",
       "4   5  0.103240  0.007701  0.004552  0.000625  0.000417  0.043724  0.036427   \n",
       "\n",
       "    Class_8   Class_9  \n",
       "0  0.015468  0.012150  \n",
       "1  0.251825  0.033253  \n",
       "2  0.034155  0.002148  \n",
       "3  0.007268  0.074479  \n",
       "4  0.397412  0.405901  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sub.to_csv('Data/submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.externals import six\n",
    "from sklearn.base import clone\n",
    "import numpy as np\n",
    "import operator\n",
    "\n",
    "\n",
    "class EnsembleClassifier(BaseEstimator, ClassifierMixin, TransformerMixin):\n",
    "\n",
    "    def __init__(self, clfs, voting='hard', weights=None, fitted = False):\n",
    "        \n",
    "        self.clfs = clfs\n",
    "        self.voting = voting\n",
    "        self.weights = weights\n",
    "        self.fitted = fitted\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        if isinstance(y, np.ndarray) and len(y.shape) > 1 and y.shape[1] > 1:\n",
    "            raise NotImplementedError('Multilabel and multi-output'\\\n",
    "                                      ' classification is not supported.')\n",
    "\n",
    "        if self.voting not in ('soft', 'hard'):\n",
    "            raise ValueError(\"Voting must be 'soft' or 'hard'; got (voting=%r)\"\n",
    "                             % voting)\n",
    "\n",
    "        if self.weights and len(self.weights) != len(self.clfs):\n",
    "            raise ValueError('Number of classifiers and weights must be equal'\n",
    "                             '; got %d weights, %d clfs'\n",
    "                             % (len(self.weights), len(self.clfs)))\n",
    "        \n",
    "\n",
    "        self.le_ = LabelEncoder()\n",
    "        self.le_.fit(y)\n",
    "        self.classes_ = self.le_.classes_\n",
    "        self.clfs_ = []\n",
    "        \n",
    "        if self.fitted == True:\n",
    "            for clf in self.clfs:\n",
    "                self.clfs_.append(clone(clf))\n",
    "            return self\n",
    "        \n",
    "        for clf in self.clfs:\n",
    "            fitted_clf = clone(clf).fit(X, self.le_.transform(y))\n",
    "            self.clfs_.append(fitted_clf)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        if self.voting == 'soft':\n",
    "\n",
    "            maj = np.argmax(self.predict_proba(X), axis=1)\n",
    "\n",
    "        else:  # 'hard' voting\n",
    "            predictions = self._predict(X)\n",
    "\n",
    "            maj = np.apply_along_axis(\n",
    "                                      lambda x:\n",
    "                                      np.argmax(np.bincount(x,\n",
    "                                                weights=self.weights)),\n",
    "                                      axis=1,\n",
    "                                      arr=predictions)\n",
    "            \n",
    "        maj = self.le_.inverse_transform(maj)\n",
    "        return maj\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "\n",
    "        avg = np.average(self._predict_probas(X), axis=0, weights=self.weights)\n",
    "        return avg\n",
    "\n",
    "    def transform(self, X):\n",
    "\n",
    "        if self.voting == 'soft':\n",
    "            return self._predict_probas(X)\n",
    "        else:\n",
    "            return self._predict(X)\n",
    "\n",
    "    def _predict(self, X):\n",
    "        if self.fitted == True:\n",
    "            return np.asarray([clf.predict(X) for clf in self.clfs]).T\n",
    "        return np.asarray([clf.predict(X) for clf in self.clfs_]).T\n",
    "\n",
    "    def _predict_probas(self, X):\n",
    "        if self.fitted == True:\n",
    "            return np.asarray([clf.predict_proba(X) for clf in self.clfs])\n",
    "        return np.asarray([clf.predict_proba(X) for clf in self.clfs_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf1 = LogisticRegression()\n",
    "clf2 = RandomForestClassifier(n_estimators=400, \n",
    "                              max_depth=15, \n",
    "                              min_samples_split=12, \n",
    "                              min_samples_leaf=3, \n",
    "                              max_features=20, \n",
    "                              n_jobs=4)\n",
    "\n",
    "clf3 = ExtraTreesClassifier(n_estimators=200, \n",
    "                            min_samples_leaf=10, \n",
    "                            max_features=30, \n",
    "                            criterion=\"gini\", \n",
    "                            min_samples_split=3, \n",
    "                            n_jobs=4)\n",
    "\n",
    "essemble_classifier = EnsembleClassifier(clfs=[clf1, clf2, clf3], voting='soft')\n",
    "essemble_classifier.fit(train, target)\n",
    "predicted = essemble_classifier.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted = essemble_classifier.predict_proba(test)\n",
    "sub = pd.DataFrame(predicted, columns = ['Class_1','Class_2','Class_3','Class_4','Class_5','Class_6','Class_7','Class_8','Class_9'])\n",
    "sub[\"id\"] = list(range(1,len(sub.index) + 1))\n",
    "cols = sub.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "sub = sub[cols]\n",
    "sub.to_csv('Data/submission.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
